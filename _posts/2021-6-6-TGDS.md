---
layout: post
title: Theory-guided Data Science
---

## Paper Information

**Title:** Theory-guided Data Science: A New Paradigm for Scientific Discovery from Data

**Authors:** Anuj Karpatne, Gowtham Atluri, James H. Faghmous, Michael Steinbach, Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, and Vipin Kumar

**Journal:** IEEE Transactions on Knowledge and Data Engineering

**Pages:** 2318-2331

**Year:** 2017

**Link:** [https://arxiv.org/pdf/1612.08544.pdf](https://arxiv.org/pdf/1612.08544.pdf)

## Paper Summary

*Theory-guided Data Science: A New Paradigm for Scientific Discovery from Data* discusses various combinations of data science techniques with scientific theory. Unlike the scientific method of discovering new knowledge, including formulating hypotheses and validating them with observed data, data science techniques use observed data to directly create new knowledge. Though data science has been successful in other fields, the authors claim that the level of success within scientific disciplines has been stifled by limited data and complex underlying phenomena. In order to develop relevant data science models that may complement scientific theory, they propose including *physical consistency* as part of measuring model performance. They then lay out a number of techniques to achieve physical consistency, modifying various stages of data science model use and development. Finally, they extend these modifications to improve the use and development of theoretical models.

First, they present modifying data science model design, constraining it to only output consistent results. They present two methods to constrain models: selecting model types that accurately represent a probability distribution of observed data, and redesigning the architecture of a model. By making these upfront modifications, they claim that models may become more generalizable.

Second, they discuss modifications to the learning (or training) algorithm. Before the training starts for the entire model, they first suggest initializing parameters, including pretraining Artificial Neural Networks (ANNs) with simpler tasks. For Bayesian Networks, they discuss using probability estimation, along with using priors learned from scientific theory. In constrained optimization problems, they propose techniques for representing complex Partial Differential Equations to model a wide array of physical phenomena, using density functional theory methods to model particle-based problems, and using additional relevant data to classification problems. Finally, to learn simpler models, specific regularization techniques, such as group Lasso techniques for attributes and multi-task learning, are introduced to additionally contribute to model consistency.

Third, they address modifications to model output. In the case where there is explicit knowledge, typically when relationships between inputs and outputs may exist in the form of equations, checking results of  data science models against theoretical models proves to be quite effective. For the implicit case, which is applicable to tasks such as computer vision, they claim that previously existing knowledge can both refine and be refined by outputs from data science models.

Fourth, they propose methods of combining theoretical and data science models, each occurring at different stages of model development. One one hand, they introduce using theoretical results as part of the inputs into the data science model. On the other hand, they discuss using data science model outputs as a form of input into theoretical models.

Finally, methods for changing theoretical models using data science models is discussed. From taking advantage of localized data by assimilating it, to calibrating theoretical models using Monte Carlo techniques, they show how theoretical model performance can be improved, while maintaining physical consistency, using data.